# Gradient-Descent-Algorithms
A collection of various gradient descent algorithms implemented in Python from scratch

# Introduction
Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847. 

# Gradient Descent Algorithms
1) Basic Gradient Descent
2) Gradient Descent with Decaying Learning Rate
3) Line Search Gradient Descent
4) Momentum Based Gradient Descent Algorithm
5) Nesterov Accelerated Gradient Descent Algorithm
6) RMSProp
7) Adagrad
8) Adam
